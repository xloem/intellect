OpenAI Baselines and Gym are a collection of existing learning parts and reinforcement scenarios for them.
They should be a reasonable seed to rapidly grow real learning.

1. Installing baselines can take a while for now due to tensorflow and specifics of versioning.
   Mark off where you are to resume later.

    $ export TMPDIR=/drive/with/lots/of/space

    # openai-baseslines uses tensorflow
    # master branch uses tensorflow 1.4 to 1.14, there is tf2 branch for newer tensorflow
    # tensorflow has 3 options: old-cpu, AVX cpu, and CUDA.

    # i might try the tf2 branch at this point.  for now below is tensorflow 1.14
    # on arm, aarch64 support is only really present in later tensorflow

    # for non-AVX cpu, recommend building from source (v1.5 is supposed to work, but doesn't seem to be supported in baselines?)
			(NOTE: mujoco requires AVX, so many and new demos won't work)
		## NOTE: can skip to pip3 install step with this file on ubuntu 18 x86_64:
		## https://siasky.net/AABpODF-l43-tZHMsgSDrSX-brTmHY5f1eaqS-nWgxGEgQ/tensorflow-1.14.1-cp36-cp36m-linux_x86_64.whl
		## rebundled on ubuntu 20:
		## https://siasky.net/AADMTsyk0kZO-BNZAgu6KQFBclGVvxsH8ZTc3qM2icshOA/tensorflow-1.14.1-cp38-cp38-linux_x86_64.whl
		## vs 1.15 is used by storybro and aidungeon
        $ git clone https://github.com/tensorflow/tensorflow.git --branch r1.14
        $ pip3 install pip six numpy wheel setuptools mock 'future>=0.17.1'
        $ pip3 install keras_applications --no-deps
        $ pip3 install keras_preprocessing --no-deps

	# INSTALL BAZEL ON NON-MAINSTREAM CPU
	$ sudo apt-get install build-essential openjdk-8-jdk python zip unzip libopenmpi-dev
	$ wget https://github.com/bazelbuild/bazel/releases/download/0.26.1/bazel-0.26.1-dist.zip
	$ mkdir bazel-0.26.1
	$ cd bazel-0.26.1
	$ unzip ../bazel-0.26.1-dist.zip
	$ env EXTRA_BAZEL_ARGS="--host_javabase=@local_jdk//:jdk" bash ./compile.sh
	$ sudo cp output/bazel /usr/local/bin/

	# INSTALL BAZEL ON MAINSTREAM CPU
        $ go get github.com/bazelbuild/bazelisk
        $ ln -s bazelisk "$(go env GOPATH)/bin/bazel"
        $ export PATH="$PATH":"$(go env GOPATH)/bin"
        $ export USE_BAZEL_VERSION=0.26.1

        # now we have bazel we can build tensorflow
        $ cd tensorflow

	# python 3.8 MAY NEED A PATCH to tensorflow: https://github.com/tensorflow/tensorflow/issues/34197
	# patch for 1.14: https://siasky.net/AACaNPKt5cFfdOxej6IfKCASmQ080kULb82Vsz9twUnTnw/tensorflow_r1.14_python3.8.patch
	# patch for 1.15: https://siasky.net/AACMLiHk3zXL0-CR5qnXTTRCIBRDG5-F8jGnJXthR1xx8w/tensorflow_r1.15_python3.8.patch

	# glibc 2.30 also needs a patch:
	# https://siasky.net/AAC7SeCUauJkKsrsNXYmT3iGsyVpzD2DP2ja7nYkzD-1MQ/tensorflow_r1.14_grpc_glibc2.30.patch
	# https://siasky.net/AADkaUr2CEx2WvddFUKRTpf1COI1AXzvMyMOy34GxEUX6w/tensorflow_r1.15_grpc_glibc2.30.patch

	# the ram_utilization_factor percentage or local_ram_resources MB will reduce thrashing and oom-killing on small systems
        #$ bazel build --config=opt --config=monolithic --ram_utilization_factor=10 //tensorflow/tools/pip_package:build_pip_package
        $ bazel build --config=opt --config=monolithic --verbose_failures --local_ram_resources=256 //tensorflow/tools/pip_package:build_pip_package

	# then tensorflow is put into a pip package
	$ bazel-bin/tensorflow/tools/pip_package/build_pip_package $TMPDIR/tensorflow-pip

	$ pip3 install $TMPDIR/tensorflow-pip/*.whl

	# LEFT OFF RIGHT HERE

    # for avx-supporting cpu, latest binaries should work
        $ pip3 install tensorflow==1.14
    # for cuda
        $ pip3 install tensorflow-gpu==1.14

    # this seemed needed for me
	$ pip3 uninstall tensorflow-tensorboard

    $ git clone https://github.com/openai/gym.git openai-gym
    $ git clone https://github.com/openai/baselines.git openai-baselines
    $ sudo apt-get install libopenblas-dev liblapack-dev cmake libopenmpi-dev python3-dev zlib1g-dev python3-keras
    $ pip3 install cython
    $ pip3 install cloudpickle==1.2

    $ cd openai-baselines
    $ pip3 install -e .

    $ cd openai-gym
    $ pip3 install -e .    # slow
    $ # pip3 install -e '.[all]' # needs some custom presetup for mujoco, a proprietary physics lib

```
python3 -m baselines.run --alg=<name of algorithm> --env=<environment_id> [additional arguments]
```

```
import gym

env = gym.make('CartPole-v0') # environment a possible life can learn in

integer_space = gym.spaces.Discrete(8) # 0-7
```

2.
 - [ ] find a way to use existing algorithms
[we want to generalize 1 learning algorithm that can possibly encompasses all the others, and write it in terms of itself.]
	[then we can use the others to grow it to live.]
 - [ ] PROPOSAL: make an algorithm that learns to use another algorithm well based on a classifier
       OR make an algorithm that picks among the other algorithms
       ALTERNATIVELY or FIRST: just choose 1 good algorithm to use for start
 - [ ] use meta- and planning-part- environments to produce a general-purpose algorithm
	stuck? just work to contribute a meta- or planning-part- environment to their repo.

# Ideas
	Decisions for branches that need to learn 'fluidly' could try all paths but use fluid choices to weigh results.
		There are other options, such as rethinking branching to have fluid space between control paths.

# Lifes (learning algorithms TODO add baselines) #
	baselines/baselines/ddpg (--alg=ddpg) (adds parameter noise to ddpg? likely an improvement over deepq)
		https://arxiv.org/abs/1509.02971 Jul 2019
	baselines/baselines/her (--alg=her) (FEW STEPS) (combines with ddpg?)
		hindsight experience replay
		https://arxiv.org/abs/1707.01495 Feb 2018
			HER uses a new kind of gym environment: GoalEnv
			The observations from these environments can be converted to the old format using
			   gym.wrappers.FlattenDictWrapper
			See https://openai.com/blog/ingredients-for-robotics-research/
	baselines/baselines/ppo1 (--alg=ppo?) (simpler than TRPO, handles more tasks than ACER,ACKTR)
		proximal policy optimization
		PPO is a general purpose, easy-to-tune algorithm used for most work at openai-baselines
		https://arxiv.org/abs/1707.06347 Aug 2017
	baselines/baselines/ppo2 (--alg=ppo2) (ppo1 optimized for gpu? may also reduce needed steps?)
	baselines/baselines/acktr (--alg=acktr)
		an improvement on a2c and trpo
		https://arxiv.org/abs/1708.05144 Aug 2017
	baselines/baselines/acer (--alg=acer)
		https://arxiv.org/abs/1611.01224 Jul 2017
	baselines/baselines/a2c (--alg=a2c) (acktr is improvement for most scenarios)
		synchronous deterministic variant of a3c (asynchronous advantage actor critic) that appears to be equal
		https://arxiv.org/abs/1602.01783
	baselines/baselines/gail
		generative adversarial imitation learning
		https://arxiv.org/abs/1606.03476 Jun 2016
	baselines/baselines/trpo_mpi (--alg=trpo_mpi) (more complex than ppo)
		https://arxiv.org/abs/1502.05477
	baselines/baselines/deepq (--alg=deepq) (variants of dqn, the first algorithms released by baselines)
		https://openai.com/blog/openai-baselines-dqn/
	RandomAgent gym/examples/agents/random_agent.py
			picks random valid step at all times
	cem.py gym/examples/agents/em.py
			cross-entropy life, defaults to 10 iterations of 25 episodes considering the top 20% 'elite'
	dqn https://github.com/sherjilozair/daqn
			DQN with Keras/Theano neural networks
	Simple DQN https://github.com/tambetm/simple_dqn
			simple, fast, and easy-to-extend DQN using Neon deep learning library, comes with tools
	AgentNet https://github.com/yandexdataschool/AgentNet.git
			provides for development of custom reinforcement neural network learning using theano/lasagne
	rllab https://github.com/rllab/rllab.git
			framework for development & evaluation of reinforcement learning algorithms, has many implemented
	keras-rl https://github.com/matthiasplappert/keras-rl.git
			implements some state-of-the-art deep reinforcement learning, uses Keras

# Environments #
	not read yet: https://github.com/openai/gym/blob/master/docs/creating-environments.md
Ways:
	env = gym.make('environment-name')
	env.reset()
	env.render(mode='human') # render a frame, 'human' makes graphics

	behavior_space = env.action_space
	observation, reward = env.step(behavior_space.sample())

	env.close() # end
Premade:
	CartPole-v0: try to balance a pole on a moving cart
	MountainCar-v0: try to drive a car up a mountain
	Acrobat-v1: try to swing two-link robot against gravity
	MountainCarContinuous-v0: drive up hill, sounds like fluid control
	Pendulum-v0: swing pendulum against gravity
	Copy-v0: move a write head and write preset data with it accurately
	DuplicatedInput-v0: remove repetition from data, with write-head
	RepeatCopy-v0: write-head, write data multiple times
	Reverse-v0: write-head, reverse data
	ReversedAddition-v0: add multi-digit numbers
	ReversedAddition3-v0: add 3 multi-digit numbers
	gyminventory: discrete state and action spaces re inventory: https://github.com/paulhendricks/gym-inventory
	gym-maze: maze navigation: https://github.com/tuzzer/gym-maze
	gym-minigrid: gridworld with minimal dependencies: https://github.com/maximecb/gym-minigrid (also miniworld)
	gym-anytrading: trading algorithms https://github.com/AminHP/gym-anytrading
	gymgo: 'go' game https://github.com/aigagror/GymGo
	atari: a number of games included, requires `pip install -e '.[atari]'` and cmake
			SpaceInvaders-v0
	robotics, physics, etc excluded from above list
	=> do not see any meta-tasks or significant planning-parts in environment list <=

# Spaces #
	Discrete(n): contains integers from 0 to n-1
	Box(n): contains n floats (coordinates in an n-dimensional box)
		box.high and box.low give the extent coordinates
space.sample(): get a random value
space.contains(v): True if v is within space
	note: transforms:
		Discretize (continuous -> discrete)
		Flatten (make 1d)
		Rescale (change continuous space size)
		https://github.com/ngc92/space-wrappers
		

= monologue =

2020-04-05 01:10 ET
not sure how to get myself to find this again, maybe putting in starts/ will be better anyway.

okay, i'm waiting on the slow dependency install now.   it's 5:23 UTC .

the demo displayed the attempt to balance a pole, but it went offscreen before end, and didn't respond to hitting X.
it turns out the demo is specifically designed to only make random actions.

step returns four values:
	- observation (object), data from environment
	- reward (float), degree of success from last action
	- done (boolean), true when environment no longer navigable
	- info (dict), diagnostic debugging information


https://gym.openai.com/docs <- I have completed this page, it did not describe what I need.  it was labeled 'getting started'.
	it also has 'environments, I'm reviewing them quickly.
they have existing environments for computations.  they appear focused on getting fluid algorithms to behave discretely.
i think it could be done more efficiently.  seems like a good idea to review.  handling need-to-poop-suddenly.

I'm reviewing the copy-v0 environment to learn.  It's an 'algorithmic' environment: gym.envs.algorithmic.
	The target is simply the input data.
	actions for algorithm envs:
		move tape read head along 1d or 2d axis
		choose: write / no-write
		data: characters to write
		difficulty increases after repeated success
	
something's going wrong on the computer i'm using to learn this.  ignoring ... there it kinda goes.  some windows overlayed on
each other.

i'm now reviewing the readme instead of the docs website.
