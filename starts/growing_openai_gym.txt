
OpenAI Gym is a collection of existing learning parts and reinforcement scenarios for them.
It should be a reasonable seed to rapidly grow real learning.

1.
    $ git clone https://github.com/openai/gym.git openai-gym
    $ cd openai-gym
    $ sudo apt-get install libopenblas-dev liblapack-dev
    $ pip3 install cython
    $ pip3 install -e .    # slow

```
import gym

env = gym.make('CartPole-v0') # environment a possible life can learn in

integer_space = gym.spaces.Discrete(8) # 0-7
```

[ ] find a way to use existing algorithms
[ ] use meta- and planning-part- environments to produce a general-purpose algorithm
	stuck? just work to contribute a meta- or planning-part- environment to their repo.


# Environments #
Ways:
	env = gym.make('environment-name')
	env.reset()
	env.render() # graphics

	behavior_space = env.action_space
	observation, reward = env.step(behavior_space.sample())

	env.close() # end
Premade:
	CartPole-v0: try to balance a pole on a moving cart
	MountainCar-v0: try to drive a car up a mountain
	Acrobat-v1: try to swing two-link robot against gravity
	MountainCarContinuous-v0: drive up hill, sounds like fluid control
	Pendulum-v0: swing pendulum against gravity
	Copy-v0: move a write head and write preset data with it accurately
	DuplicatedInput-v0: remove repetition from data, with write-head
	RepeatCopy-v0: write-head, write data multiple times
	Reverse-v0: write-head, reverse data
	ReversedAddition-v0: add multi-digit numbers
	ReversedAddition3-v0: add 3 multi-digit numbers
	gyminventory: discrete state and action spaces re inventory: https://github.com/paulhendricks/gym-inventory
	gym-maze: maze navigation: https://github.com/tuzzer/gym-maze
	gym-minigrid: gridworld with minimal dependencies: https://github.com/maximecb/gym-minigrid (also miniworld)
	gym-anytrading: trading algorithms https://github.com/AminHP/gym-anytrading
	gymgo: 'go' game https://github.com/aigagror/GymGo
	atari: a number of games included, requires `pip install -e '.[atari]'` and cmake
			SpaceInvaders-v0
	robotics, physics, etc excluded from above list
	=> do not see any meta-tasks or significant planning-parts in environment list <=

# Spaces #
	Discrete(n): contains integers from 0 to n-1
	Box(n): contains n floats (coordinates in an n-dimensional box)
		box.high and box.low give the extent coordinates
space.sample(): get a random value
space.contains(v): True if v is within space
		

= monologue =

2020-04-05 01:10 ET
not sure how to get myself to find this again, maybe putting in starts/ will be better anyway.

okay, i'm waiting on the slow dependency install now.   it's 5:23 UTC .

the demo displayed the attempt to balance a pole, but it went offscreen before end, and didn't respond to hitting X.
it turns out the demo is specifically designed to only make random actions.

step returns four values:
	- observation (object), data from environment
	- reward (float), degree of success from last action
	- done (boolean), true when environment no longer navigable
	- info (dict), diagnostic debugging information


https://gym.openai.com/docs <- I have completed this page, it did not describe what I need.  it was labeled 'getting started'.
	it also has 'environments, I'm reviewing them quickly.
they have existing environments for computations.  they appear focused on getting fluid algorithms to behave discretely.
i think it could be done more efficiently.  seems like a good idea to review.  handling need-to-poop-suddenly.

I'm reviewing the copy-v0 environment to learn.  It's an 'algorithmic' environment: gym.envs.algorithmic.
	The target is simply the input data.
	actions for algorithm envs:
		move tape read head along 1d or 2d axis
		choose: write / no-write
		data: characters to write
		difficulty increases after repeated success
	
